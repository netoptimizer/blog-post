#+Title: New introduction to blogpost

Abbreviations:
- RSS = receive side scaling
- RPS = Receive Packet Steering
- XDP = eXpress Data Path

* Introduction

Modern Network Interface Cards (NIC) scale by having a hardware feature,
named Receive Side Scaling (RSS), that based on a flow-hash spread incoming
traffic across the RX irq-lines, which can be handled by different CPUs.
Unfortunately there can be a number of situations where the NIC hardware RSS
features fail, which result in delivering all packets to same RX IRQ-line
and thus same CPU.

This blogpost is about how to handle this situation in software, when RSS
fails, with a strong focus on how to solve this *using XDP and CPU-map*
redirect feature.

* Existing software receive steering

The Linux kernel already have a software feature called Receive Packet
Steering (RPS) and Receive Flow Steering (RFS), which is logically a
software implementation of RSS. This feature is both hard to configure ([1])
and have limited scalability and performance.

[1] https://www.kernel.org/doc/html/latest/networking/scaling.html

The performance problem is because RPS and RFS, happens too late in the
kernels receive path, most importantly after the allocation of the "SKB"
metadata kernel object that keeps track of the packet. Transferring and
queuing these SKB-objects to a remote CPU is also a cross-CPU scalability
bottleneck. That involves Inter Processor Communication calls and moving
cache-lines between CPU. (Details: The kernels slab memory allocator is also
challenged as the per-CPU slab caches loose their effect).

* Faster software receive steering with XDP

A faster and more scaleable software solution is using XDP to redirect
raw-frames into a CPU-map. XDP is a kernel layer before the normal network
stack. This means it runs before allocating the mentioned SKB object, and
generally avoiding any per-packet memory allocations.

* What XDP actions

XDP is an eBPF-program that run at the earliest possible point in the driver
receive-path when DMA rx-ring is synced for the CPU.

This eBPF-program parse the received frames and returns a verdict or action.
The actions are:
 1) XDP_DROP - drop frame, which at driver level means reuse without alloc.
 2) XDP_PASS - let it pass for normal network stack handling.
 3) XDP_TX - Bounce packet out same interface.
 4) XDP_REDIRECT - the advanced action this blogpost focus on.

** What makes XDP_REDIRECT special?

The XDP_REDIRECT action is different, because it can queue XDP frame
(xdp_frame) objects into a BPF-map. All the other actions need to take
immediate action, because the (xdp_buff) data-structure that keeps track
packet-data is not-allocated anywhere, it is simply a variable in the
function call itself.

It is essential for performance to avoid per-packet allocations. When
XDP-redirecting the xdp_buff object is converted into a xdp_frame object to
allow queuing this object. To avoid any memory allocations, the xdp_frame
object is placed in the top headroom of the data-packet itself. (Details: A
CPU prefetch operation, runs before the XDP BPF-prog, which hides the
overhead of writing into this cache-line).

The XDP BPF-prog returns action XDP_REDIRECT, but prior to this it have
called one of these two BPF-helpers, to describe the redirect *destination*
where the frame should be redirected to:

- bpf_redirect(ifindex, flags)
- bpf_redirect_map(bpf_map, index_key, flags)

The first helper is simply choosing the Linux net device destination via the
ifindex as key. The second helper is the big leap that allow us to extend
XDP-redirect. This helper can redirect into a BPF-map with at a specific
index_key. As we will explore in the next section, this flexibility can be
used for CPU steering. The map-redirect is also responsible for creating a
bulk effect (as drivers are required to call a xdp_flush operation when
NAPI-poll budget ends).

* Redirecting into a CPU-map

The BPF-maps concept is a generic key-value store with different types. It
is the main interface between a user-space application and a eBPF program
running in the kernel. The concept seems simple, but also very flexible as
it is up-to each individual BPF map-type to define the meaning of the key
and value. As of this writing there are 28 different map types[2].

[2] https://elixir.bootlin.com/linux/v5.10-rc2/source/include/uapi/linux/bpf.h#L130

For our use-case of software CPU receive steering, the CPUMAP type
(BPF_MAP_TYPE_CPUMAP) is what we need. The CPUMAP represent the CPUs in the
system (zero) indexed as the map-key, and the map-value is the config
settings (for this CPU). (Hint: more about how we extended the map-value for
adding new features later in blogpost).

